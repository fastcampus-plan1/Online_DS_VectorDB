{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from transformers import YolosFeatureExtractor, YolosForObjectDetection\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import openai\n",
    "import os\n",
    "\n",
    "# Ignore warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "# pandas dataframe display\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize openai\n",
    "os.environ['OPENAI_API_KEY']= \"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\"\n",
    "openai.api_key = os.environ[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 목차\n",
    "\n",
    "### 1. yolo를 활용한 object detection\n",
    "### 2. bbox 통합\n",
    "### 3. cropping\n",
    "### 4. 서치 가능한 이미지 여부 판단\n",
    "### 5. 발견한 각 카테고리 별 서치 결과 제공"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attributes = pd.read_csv(\"attribute_specific.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from yolo_utils import fix_channels, visualize_predictions, rescale_bboxes, plot_results, box_cxcywh_to_xyxy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"valentinafeve/yolos-fashionpedia\"\n",
    "\n",
    "feature_extractor = YolosFeatureExtractor.from_pretrained('hustvl/yolos-small')\n",
    "model = YolosForObjectDetection.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# 미리 선정된 prediction labels\n",
    "cats = ['shirt, blouse', 'top, t-shirt, sweatshirt', 'sweater', 'cardigan', 'jacket', 'vest', 'pants', 'shorts', 'skirt', 'coat', 'dress', 'jumpsuit', 'cape', 'glasses', 'hat', 'headband, head covering, hair accessory', 'tie', 'glove', 'watch', 'belt', 'leg warmer', 'tights, stockings', 'sock', 'shoe', 'bag, wallet', 'scarf', 'umbrella', 'hood', 'collar', 'lapel', 'epaulette', 'sleeve', 'pocket', 'neckline', 'buckle', 'zipper', 'applique', 'bead', 'bow', 'flower', 'fringe', 'ribbon', 'rivet', 'ruffle', 'sequin', 'tassel']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_PATH = 'test_images/test_image5.jpg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = Image.open(open(IMAGE_PATH, \"rb\"))\n",
    "image = fix_channels(ToTensor()(image))\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = feature_extractor(images=image, return_tensors=\"pt\")\n",
    "outputs = model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_predictions(image, outputs, threshold=0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probas = outputs.logits.softmax(-1)[0, :, :-1]\n",
    "len(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def idx_to_text(i):\n",
    "    return cats[i]\n",
    "\n",
    "probas = outputs.logits.softmax(-1)[0, :, :-1]\n",
    "keep = probas.max(-1).values > 0.5\n",
    "\n",
    "prob = probas[keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probas.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probas[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = [np.argmax(idx.detach().numpy()) for idx in prob]\n",
    "\n",
    "indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detected_cats = [cats[idx] for idx in indices]\n",
    "\n",
    "detected_cats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(outputs.pred_boxes[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boxes = outputs.pred_boxes[0, keep].cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bboxes_scaled = rescale_bboxes(boxes, image.size).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bboxes_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLORS = [[0.000, 0.447, 0.741], [0.850, 0.325, 0.098], [0.929, 0.694, 0.125],\n",
    "          [0.494, 0.184, 0.556], [0.466, 0.674, 0.188], [0.301, 0.745, 0.933]]\n",
    "\n",
    "def plot_results_2(pil_img, labels, boxes):\n",
    "    plt.figure(figsize=(16,10))\n",
    "    plt.imshow(pil_img)\n",
    "    ax = plt.gca()\n",
    "    colors = COLORS * 100\n",
    "    for label, (xmin, ymin, xmax, ymax), c in zip(labels, boxes, colors):\n",
    "        ax.add_patch(plt.Rectangle((xmin, ymin), xmax - xmin, ymax - ymin,\n",
    "                                   fill=False, color=c, linewidth=3))\n",
    "        \n",
    "        ax.text(xmin, ymin, label, fontsize=10,\n",
    "                bbox=dict(facecolor=c, alpha=0.8))\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results_2(image, detected_cats, bboxes_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### filter bounding boxes (필요한 카테고리만 선별)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = pd.read_csv(\"clothes_final2.csv\")\n",
    "new_df.name.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_of_interest = new_df.name.unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_of_interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_indices = list()\n",
    "keep_bboxes = list()\n",
    "\n",
    "for idx, box in zip(detected_cats, bboxes_scaled):\n",
    "    if idx in category_of_interest:\n",
    "        keep_indices.append(idx)\n",
    "        keep_bboxes.append(box)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_indices, keep_bboxes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### concat bboxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iou(boxA, boxB):\n",
    "    # Calculate the intersection rectangle\n",
    "    xA = max(boxA[0], boxB[0])\n",
    "    yA = max(boxA[1], boxB[1])\n",
    "    xB = min(boxA[2], boxB[2])\n",
    "    yB = min(boxA[3], boxB[3])\n",
    "\n",
    "    # Compute the area of intersection\n",
    "    interArea = max(0, xB - xA) * max(0, yB - yA)\n",
    "\n",
    "    # Compute the area of both the prediction and ground-truth rectangles\n",
    "    boxAArea = (boxA[2] - boxA[0]) * (boxA[3] - boxA[1])\n",
    "    boxBArea = (boxB[2] - boxB[0]) * (boxB[3] - boxB[1])\n",
    "\n",
    "    # Compute the intersection over union by taking the intersection\n",
    "    # area and dividing it by the sum of prediction + ground-truth\n",
    "    # areas - the intersection area\n",
    "    iou = interArea / float(boxAArea + boxBArea - interArea)\n",
    "\n",
    "    return iou\n",
    "\n",
    "def merge_boxes(boxes, labels):\n",
    "    merged_boxes = []\n",
    "    merged_labels = []\n",
    "    used = set()\n",
    "\n",
    "    for i in range(len(boxes)):\n",
    "        if i in used:\n",
    "            continue\n",
    "        current_box = boxes[i]\n",
    "        for j in range(i + 1, len(boxes)):\n",
    "            if j in used or labels[i] != labels[j]:\n",
    "                continue\n",
    "            if iou(current_box, boxes[j]) > 0.5:  # Assuming a positive IoU indicates overlap\n",
    "                # For xyxy format, we merge by finding the min and max coordinates\n",
    "                current_box = [\n",
    "                    min(current_box[0], boxes[j][0]), \n",
    "                    min(current_box[1], boxes[j][1]), \n",
    "                    max(current_box[2], boxes[j][2]), \n",
    "                    max(current_box[3], boxes[j][3])\n",
    "                ]\n",
    "                used.add(j)\n",
    "        merged_boxes.append(current_box)\n",
    "        merged_labels.append(labels[i])\n",
    "        used.add(i)\n",
    "\n",
    "    return np.array(merged_boxes), merged_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results_2(image, keep_indices, keep_bboxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_bbox, merged_labels = merge_boxes(keep_bboxes, keep_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_labels, merged_bbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLORS = [[0.000, 0.447, 0.741], [0.850, 0.325, 0.098], [0.929, 0.694, 0.125],\n",
    "          [0.494, 0.184, 0.556], [0.466, 0.674, 0.188], [0.301, 0.745, 0.933]]\n",
    "\n",
    "def plot_results_2(pil_img, labels, boxes):\n",
    "    plt.figure(figsize=(16,10))\n",
    "    plt.imshow(pil_img)\n",
    "    ax = plt.gca()\n",
    "    colors = COLORS * 100\n",
    "    for label, (xmin, ymin, xmax, ymax), c in zip(labels, boxes.tolist(), colors):\n",
    "        ax.add_patch(plt.Rectangle((xmin, ymin), xmax - xmin, ymax - ymin,\n",
    "                                   fill=False, color=c, linewidth=3))\n",
    "        \n",
    "        ax.text(xmin, ymin, label, fontsize=10,\n",
    "                bbox=dict(facecolor=c, alpha=0.8))\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results_2(image, merged_labels, merged_bbox)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## crop images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from image_utils import crop_bbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageFilter\n",
    "\n",
    "def resize_img(image, category):\n",
    "    standard_size = {\"lowerbody\":[420, 540],\n",
    "        \"upperbody\":[500, 700],\n",
    "        \"wholebody\":[480, 880],\n",
    "        \"legs and feet\":[100, 150],\n",
    "        \"head\":[150, 100],\n",
    "        \"others\":[200, 350],\n",
    "        \"waist\":[200, 100],\n",
    "        \"arms and hands\":[75, 75],\n",
    "        \"neck\":[120, 200]}\n",
    "    \n",
    "    w, h = image.size\n",
    "    img_size = w*h\n",
    "\n",
    "    new_width, new_height = standard_size[category]\n",
    "    new_size = new_width * new_height\n",
    "\n",
    "    if img_size >= new_size:\n",
    "        # For downsizing\n",
    "        downsized_image = image.resize((new_width, new_height), Image.Resampling.LANCZOS)\n",
    "        return downsized_image\n",
    "    else:\n",
    "        # For upsizing\n",
    "        upsized_image = image.resize((new_width, new_height), Image.Resampling.LANCZOS)\n",
    "        upsized_image = upsized_image.filter(ImageFilter.UnsharpMask(radius=2, percent=150, threshold=3))\n",
    "        return upsized_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = new_df[['supercategory', 'name']].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# categories.to_csv(\"categories.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 여기서 dictionary로 변환을 하면서 각 카테고리를 키 값으로 설정해준다\n",
    "# 따라서 '신발'이 두 개 detect 되더라도 하나만 선택\n",
    "cropped_images = dict()\n",
    "\n",
    "for label, box in zip(merged_labels, merged_bbox):\n",
    "    cropped = resize_img(crop_bbox(image, box), categories.loc[categories['name']==label, 'supercategory'].values[0])\n",
    "    cropped_images[label] = cropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cropped_images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search from DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone import Pinecone\n",
    "\n",
    "pc = Pinecone(api_key=\"74e30e50-02fa-4e55-9bff-affa6a3817a0\")\n",
    "# index 개수 확인\n",
    "# index_list = pc.list_indexes().indexes\n",
    "\n",
    "# index description\n",
    "index = pc.Index(\"fastcampus\")\n",
    "index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLIP\n",
    "from image_utils import fetch_clip, extract_img_features, draw_images\n",
    "\n",
    "model, processor, tokenizer = fetch_clip(model_name=\"patrickjohncyh/fashion-clip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cropped_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = dict()\n",
    "\n",
    "for label, image in cropped_images.items():\n",
    "    img_emb = extract_img_features(image, processor, model).tolist()\n",
    "\n",
    "    result = index.query(\n",
    "        vector=img_emb[0],\n",
    "        top_k=5,\n",
    "        filter={\"category\": {\"$eq\": label}},\n",
    "        include_metadata=True\n",
    "    )\n",
    "\n",
    "    paths = [i['metadata']['img_path'] for i in result.matches]\n",
    "\n",
    "    results[label] = paths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, paths in results.items():\n",
    "    print(k)\n",
    "    draw_images([Image.open(i) for i in paths])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clothes_detector(image, feature_extractor, model, thresh=0.5):\n",
    "    # all categories\n",
    "    cats = ['shirt, blouse', 'top, t-shirt, sweatshirt', 'sweater', 'cardigan', 'jacket', 'vest', 'pants', 'shorts', \n",
    "            'skirt', 'coat', 'dress', 'jumpsuit', 'cape', 'glasses', 'hat', 'headband, head covering, hair accessory', \n",
    "            'tie', 'glove', 'watch', 'belt', 'leg warmer', 'tights, stockings', 'sock', 'shoe', 'bag, wallet', 'scarf', \n",
    "            'umbrella', 'hood', 'collar', 'lapel', 'epaulette', 'sleeve', 'pocket', 'neckline', 'buckle', 'zipper', \n",
    "            'applique', 'bead', 'bow', 'flower', 'fringe', 'ribbon', 'rivet', 'ruffle', 'sequin', 'tassel']\n",
    "    # category we are interested in\n",
    "    category_of_interest = ['pants', 'shirt, blouse', 'jacket', 'top, t-shirt, sweatshirt', 'dress', 'shoe', 'glasses', \n",
    "                        'skirt', 'bag, wallet', 'belt', 'headband, head covering, hair accessory', 'sock', 'hat', \n",
    "                        'watch', 'glove', 'tights, stockings', 'sweater', 'tie', 'shorts', 'scarf', 'coat', 'vest', \n",
    "                        'umbrella', 'cardigan', 'cape', 'jumpsuit', 'leg warmer']\n",
    "    # yolo detection\n",
    "    inputs = feature_extractor(images=image, return_tensors=\"pt\")\n",
    "    outputs = model(**inputs)\n",
    "    \n",
    "    # extract detected labels and boundingboxes\n",
    "    probas = outputs.logits.softmax(-1)[0, :, :-1]\n",
    "    keep = probas.max(-1).values > thresh\n",
    "\n",
    "    prob = probas[keep]\n",
    "\n",
    "    indices = [np.argmax(idx.detach().numpy()) for idx in prob]\n",
    "    detected_cats = [cats[idx] for idx in indices]\n",
    "    boxes = outputs.pred_boxes[0, keep].cpu()\n",
    "\n",
    "    bboxes_scaled = rescale_bboxes(boxes, image.size).tolist()\n",
    "    \n",
    "    # keep boxes that we are interested in\n",
    "    keep_indices = list()\n",
    "    keep_bboxes = list()\n",
    "\n",
    "    for idx, box in zip(detected_cats, bboxes_scaled):\n",
    "        if idx in category_of_interest:\n",
    "            keep_indices.append(idx)\n",
    "            keep_bboxes.append(box)\n",
    "    # overlapping한 구간이 있는 bbox들을 통합\n",
    "    merged_bbox, merged_labels = merge_boxes(keep_bboxes, keep_indices)\n",
    "\n",
    "    # cropping\n",
    "    categories = pd.read_csv(\"categories.csv\")\n",
    "    cropped_images = dict()\n",
    "\n",
    "    for label, box in zip(merged_labels, merged_bbox):\n",
    "        cropped = resize_img(crop_bbox(image, box), categories.loc[categories['name']==label, 'supercategory'].values[0])\n",
    "        cropped_images[label] = cropped\n",
    "\n",
    "    return cropped_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_search(index, cropped_images, model, processor, top_k=10):\n",
    "    results = dict()\n",
    "\n",
    "    for label, image in cropped_images.items():\n",
    "        img_emb = extract_img_features(image, processor, model).tolist()\n",
    "\n",
    "        result = index.query(\n",
    "            vector=img_emb[0],\n",
    "            top_k=top_k,\n",
    "            filter={\"category\": {\"$eq\": label}},\n",
    "            include_metadata=True\n",
    "        )\n",
    "\n",
    "        results[label] = result\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"valentinafeve/yolos-fashionpedia\"\n",
    "\n",
    "feature_extractor = YolosFeatureExtractor.from_pretrained('hustvl/yolos-small')\n",
    "model = YolosForObjectDetection.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from search_utils import clothes_detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_model, clip_processor, clip_tokenizer = fetch_clip(model_name=\"patrickjohncyh/fashion-clip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = Image.open(\"test_images/test.jpg\")\n",
    "image = fix_channels(ToTensor()(image))\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cropped_items = clothes_detector(image, feature_extractor, model, thresh=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cropped_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_result = image_search(index, cropped_items, clip_model, clip_processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이미지들의 path 가져오기\n",
    "paths = dict()\n",
    "for k,v in search_result.items():\n",
    "    paths[k] = [i['metadata']['img_path'] for i in v['matches']]\n",
    "\n",
    "# 이미지들 show\n",
    "for k,v in paths.items():\n",
    "    print(k)\n",
    "    draw_images([Image.open(i) for i in v])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastcampus",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
