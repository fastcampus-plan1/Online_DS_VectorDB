{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### word2vec을 이용한 단어 embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT야, simpsons 캐릭터 이름이 들어간 랜덤 문장 10개를 생성해줘\n",
    "\n",
    "sentences = [\"Homer Simpson forgot his lunch at home, so he had to buy a burger on his way to work.\",\n",
    "    \"Marge was busy knitting a new sweater for Bart's upcoming school play.\",\n",
    "    \"Lisa Simpson played a beautiful saxophone solo at the school concert.\",\n",
    "    \"Mr. Burns secretly plotted another scheme from his office at the Springfield Nuclear Power Plant.\",\n",
    "    \"Ned Flanders offered to help Homer fix the fence between their houses.\",\n",
    "    \"Bart Simpson tried a new prank at school, but it didn't go as planned.\",\n",
    "    \"Milhouse and Bart spent the afternoon playing video games and forgot to do their homework.\",\n",
    "    \"Maggie Simpson's adorable giggle filled the room as she played with her toys.\",\n",
    "    \"Apu had a busy day at the Kwik-E-Mart, dealing with a rush of customers.\",\n",
    "    \"Krusty the Clown decided to change his show a bit to attract a new audience.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing\n",
    "# get rid of stopwords, lower case\n",
    "\n",
    "sentences = [re.sub(r\"[.',]\", \"\", sentence).lower().split(\" \") for sentence in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['homer',\n",
       " 'simpson',\n",
       " 'forgot',\n",
       " 'his',\n",
       " 'lunch',\n",
       " 'at',\n",
       " 'home',\n",
       " 'so',\n",
       " 'he',\n",
       " 'had',\n",
       " 'to',\n",
       " 'buy',\n",
       " 'a',\n",
       " 'burger',\n",
       " 'on',\n",
       " 'his',\n",
       " 'way',\n",
       " 'to',\n",
       " 'work']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train word2vec\n",
    "\n",
    "skip_gram = Word2Vec(sentences, vector_size=300, min_count=1, window=5, sg=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "homer 의 vector representation : \n",
      "[-1.66068680e-03 -4.20303462e-04  1.09255337e-03 -2.18484551e-03\n",
      " -3.23499087e-03 -3.12648294e-03  3.04949097e-03  1.87829603e-03\n",
      " -1.61115616e-03 -2.80928891e-03  4.52324050e-04  9.58630932e-04\n",
      " -4.38321120e-04  4.20039461e-04 -1.44900673e-03  1.57583959e-03\n",
      "  5.10560756e-04  2.96067167e-03 -3.31212161e-03 -1.82445510e-03\n",
      " -3.01285670e-03 -1.11729649e-04 -2.61634029e-03  1.71959959e-03\n",
      " -2.11100094e-03 -2.03485345e-03  1.69092277e-03 -2.72677210e-03\n",
      "  4.76851739e-04 -2.43410910e-03  3.28659941e-03  2.89322482e-03\n",
      "  5.95899648e-04  1.94471062e-03  1.52419391e-03 -1.98301394e-03\n",
      "  3.28957522e-03 -3.26667982e-03  2.68764189e-03  9.17330268e-04\n",
      " -9.88418004e-04 -1.19753159e-03  3.04961996e-03 -1.81994250e-03\n",
      "  2.79433304e-03 -1.95553945e-03  2.79207341e-03 -1.48160951e-04\n",
      "  2.66201468e-03 -1.02206389e-03  2.01950571e-03  2.95562134e-03\n",
      "  8.11300066e-04  4.55215428e-04  1.65783032e-03  2.71425396e-03\n",
      "  2.85594515e-03  2.84643611e-03  2.36571440e-03  2.69569410e-03\n",
      "  2.86536408e-03  1.06979760e-05 -3.44244880e-04  5.48140437e-04\n",
      " -5.46397168e-05  2.81422021e-04 -2.87100067e-03 -3.18586407e-03\n",
      " -7.79178459e-04  2.93133804e-03 -1.23406888e-03 -2.29325425e-03\n",
      "  1.63925881e-03  3.19730258e-04  6.20175560e-04  1.22176390e-03\n",
      "  1.14811910e-03  1.92712888e-03  4.05051280e-04  3.37304111e-04\n",
      "  3.05107539e-03  9.27790359e-04 -1.60060322e-03  2.26521911e-03\n",
      "  1.76511216e-03  9.88276559e-04 -1.08301686e-03  1.13988691e-03\n",
      "  2.16252939e-03  2.34275823e-03  3.24678927e-04 -2.81540281e-03\n",
      "  7.21826509e-05  1.40061165e-04  1.34546531e-03 -3.10492166e-03\n",
      "  3.28488904e-03 -2.38402886e-03  1.96512206e-03 -3.11573036e-03\n",
      "  3.24020907e-03  2.45439238e-03  4.50846332e-04 -1.12356886e-03\n",
      " -1.32229543e-04  1.22046615e-04 -2.09888932e-03  1.93005183e-03\n",
      "  7.24676589e-04  1.26012217e-03 -2.42453138e-03  2.82842526e-03\n",
      "  1.41696830e-04 -8.03092407e-05 -3.03274323e-03  1.35573710e-03\n",
      "  2.25264905e-03  2.45702779e-03 -2.15250649e-03 -2.68144207e-03\n",
      " -1.84272008e-03 -1.78062328e-04 -2.73930444e-03 -2.76312162e-03\n",
      " -6.49262918e-04  3.79283010e-04 -3.19513585e-03 -1.26098550e-03\n",
      "  2.15646287e-04  2.32684077e-03  5.77375351e-04 -1.92653519e-04\n",
      " -2.47276342e-03 -2.28028302e-03 -2.15097447e-04  2.50997022e-03\n",
      "  1.80709839e-03 -5.11256221e-04  3.89818801e-04 -3.23218526e-03\n",
      " -4.55559610e-04 -1.58451556e-03  1.93722581e-03 -7.54477456e-04\n",
      " -1.55739812e-03 -3.19571234e-03 -4.31633816e-04 -2.40551936e-03\n",
      " -5.34374383e-04 -1.37561746e-03 -7.84670701e-04 -1.12861255e-03\n",
      " -2.72948877e-03 -4.06166451e-04  5.73715777e-04 -1.33450981e-03\n",
      " -2.56841327e-03 -1.21285731e-03 -3.02833086e-03 -1.85324418e-04\n",
      "  1.97609677e-03 -9.44438973e-04  1.04032643e-03  1.68447522e-03\n",
      "  2.78418534e-03  1.86675217e-03  3.20062647e-03 -3.20460391e-03\n",
      " -2.68438552e-03 -2.21391930e-03 -2.53441045e-03 -2.67933286e-03\n",
      " -2.59062601e-03 -9.35797405e-04  4.70210012e-04 -9.60970414e-04\n",
      " -2.90283281e-03  1.63133163e-03  3.08230374e-04  1.53027149e-03\n",
      "  2.39912583e-03  2.56149494e-03 -2.97559076e-04  1.21046661e-03\n",
      " -1.73652580e-03  6.26456167e-04  1.51195272e-03  3.34489834e-03\n",
      " -1.06545002e-03  9.09453316e-04 -1.90982758e-03 -7.30646891e-04\n",
      "  2.71163764e-03 -1.30748888e-03 -3.96405841e-04 -3.12943314e-03\n",
      " -3.15011223e-03  2.96484376e-03 -1.89172395e-03  1.65633566e-03\n",
      " -2.33992934e-03 -8.17773107e-04 -2.67595844e-03  2.52207834e-03\n",
      "  2.07958114e-03  1.73536746e-03  2.80222762e-03 -2.07082237e-04\n",
      " -3.09491903e-03  3.03551811e-03 -1.69026875e-03  2.57623312e-03\n",
      "  1.81319134e-03 -3.93923052e-04 -2.51527084e-03 -5.22290589e-04\n",
      "  2.01459555e-03 -2.37731100e-03  4.18179989e-04 -2.67173676e-03\n",
      "  2.90240673e-03 -9.63921542e-04  3.13616078e-03 -1.91106240e-03\n",
      " -3.27868620e-03 -2.89353030e-03 -1.34260673e-03  1.56894093e-03\n",
      " -6.50069196e-05  3.04524880e-03  1.07473449e-03  1.23249064e-03\n",
      "  1.00344489e-03  2.73522572e-03 -8.72792734e-04  2.49517243e-03\n",
      " -3.16875358e-03  9.50374117e-04 -2.13087056e-04  8.35450919e-05\n",
      "  2.30753538e-03 -9.51145252e-04 -8.14512430e-04  1.94224122e-05\n",
      " -1.65353616e-04 -1.20843516e-03  2.11570039e-03 -2.22024950e-03\n",
      "  2.63734371e-03 -1.30435237e-05  8.47343297e-04  1.09062018e-03\n",
      " -7.62929703e-05  5.49671764e-04 -1.08596124e-03  1.55842700e-03\n",
      "  9.04466433e-05 -1.12329691e-03 -2.93511851e-03 -3.35479411e-03\n",
      "  1.13948707e-04 -1.90499390e-03 -3.52673727e-04 -1.46952574e-03\n",
      " -2.92333635e-03  3.45982437e-04  2.00314121e-03 -7.07993808e-04\n",
      " -2.44983658e-03  1.04414253e-03 -1.54392270e-04 -1.81977404e-03\n",
      " -3.66952881e-04 -1.86574136e-04 -1.04066113e-03 -3.27943638e-03\n",
      "  2.55812518e-03  1.24969310e-03 -8.64422240e-04  2.39443034e-03\n",
      "  1.55546557e-04  2.37179967e-03 -5.29629004e-04  2.50017340e-03\n",
      " -1.58586954e-05 -1.98490988e-03 -1.60756963e-03  3.23407492e-03\n",
      "  1.77424648e-04  2.97734747e-04  2.80875596e-03 -2.06154329e-03\n",
      " -5.77152066e-04 -2.73376517e-03 -2.23413017e-03 -2.84535927e-03\n",
      "  1.31321000e-03  9.17847501e-04  1.90934294e-03  8.00744805e-04]\n"
     ]
    }
   ],
   "source": [
    "print(\"{} 의 vector representation : \\n{}\".format('homer', skip_gram.wv.get_vector(skip_gram.wv.key_to_index['homer'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('marge', 0.14081519842147827),\n",
       " ('offered', 0.13243569433689117),\n",
       " ('games', 0.12250108271837234),\n",
       " ('her', 0.11486156284809113),\n",
       " ('nuclear', 0.10569248348474503),\n",
       " ('do', 0.09913019835948944),\n",
       " ('toys', 0.0984482392668724),\n",
       " ('office', 0.0924413651227951),\n",
       " ('bart', 0.09009940177202225),\n",
       " ('way', 0.08802291750907898)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skip_gram.wv.most_similar(\"homer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "직접 유사도 구해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "homer_vector = skip_gram.wv.get_vector(skip_gram.wv.key_to_index['homer'])\n",
    "marge_vector = skip_gram.wv.get_vector(skip_gram.wv.key_to_index['marge'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 유사도 계산하기 from scratch\n",
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "\n",
    "def cosine_similarity(vector_a: np.ndarray, vector_b: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    두 벡터간 cosine similarity를 계산\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    vector_a : np.ndarray\n",
    "        The first input vector.\n",
    "    vector_b : np.ndarray\n",
    "        The second input vector.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        The cosine similarity between `vector_a` and `vector_b`, which is a value between -1 and 1.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    dot_product = np.dot(vector_a, vector_b)\n",
    "    norm_a = norm(vector_a)\n",
    "    norm_b = norm(vector_b)\n",
    "    similarity = dot_product / (norm_a * norm_b)\n",
    "    return similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.14081518"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(homer_vector, marge_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simpsons dataset을 활용한 Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://images.edrawmax.com/what-is/simpsons-family-tree/example.png) <br>\n",
    "출처 : https://images.edrawmax.com/what-is/simpsons-family-tree/example.png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/gieunkwak/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/gieunkwak/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import Optional\n",
    "import nltk\n",
    "import re\n",
    "import pandas as pd\n",
    "import spacy\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(158314, 2)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('simpsons_dataset.csv')\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>raw_character_text</th>\n",
       "      <th>spoken_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Miss Hoover</td>\n",
       "      <td>No, actually, it was a little of both. Sometim...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Lisa Simpson</td>\n",
       "      <td>Where's Mr. Bergstrom?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Miss Hoover</td>\n",
       "      <td>I don't know. Although I'd sure like to talk t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Lisa Simpson</td>\n",
       "      <td>That life is worth living.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Edna Krabappel-Flanders</td>\n",
       "      <td>The polls will be open from now until the end ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        raw_character_text                                       spoken_words\n",
       "0              Miss Hoover  No, actually, it was a little of both. Sometim...\n",
       "1             Lisa Simpson                             Where's Mr. Bergstrom?\n",
       "2              Miss Hoover  I don't know. Although I'd sure like to talk t...\n",
       "3             Lisa Simpson                         That life is worth living.\n",
       "4  Edna Krabappel-Flanders  The polls will be open from now until the end ..."
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "raw_character_text    17814\n",
       "spoken_words          26459\n",
       "dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"No, actually, it was a little of both. Sometimes when a disease is in all the magazines and all the news shows, it's only natural that you think you have it.\""
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[0, 'spoken_words']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lemmatize and remove the stopwords and non-alphabetic characters for each line of dialogue\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm', disable=['ner', 'parser'])\n",
    "\n",
    "def cleaning(doc):\n",
    "    \"\"\"\n",
    "    Cleans a spaCy Doc object by lemmatizing its tokens and removing stop words,\n",
    "    then joins the remaining tokens into a single string if there are more than two tokens left.\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    doc : spacy.tokens.Doc\n",
    "        A spaCy Doc object containing the processed text.\n",
    "    \n",
    "    Returns:\n",
    "    ----------\n",
    "    Optional : str\n",
    "        A string composed of the lemmatized, non-stop tokens separated by spaces,\n",
    "        if the resulting list of tokens has more than two elements. Otherwise, returns None.\n",
    "    \"\"\"\n",
    "\n",
    "    txt = [token.lemma_ for token in doc if not token.is_stop]\n",
    "    if len(txt) > 2:\n",
    "        return ' '.join(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only keep alphabets\n",
    "cleaner = (re.sub(\"[^A-Za-z']+\", ' ', str(row)).lower() for row in df['spoken_words'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = [cleaning(doc) for doc in nlp.pipe(cleaner, batch_size=5000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'actually little disease magazine news show natural think'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(85956, 1)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dataframe에 넣어서 null이 있는 대화는 삭제\n",
    "# 주로 null은 특정 행동을 했지만 대화가 없었을 때임\n",
    "\n",
    "df_clean = pd.DataFrame({'clean': txt})\n",
    "df_clean = df_clean.dropna().drop_duplicates()\n",
    "df_clean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 하나의 문장을 여러 단위의 단어로 분할\n",
    "sentences = [s.split(' ') for s in df_clean['clean']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "85956"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['actually',\n",
       " 'little',\n",
       " 'disease',\n",
       " 'magazine',\n",
       " 'news',\n",
       " 'show',\n",
       " 'natural',\n",
       " 'think']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word2Vec 모델 훈련"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# help(Word2Vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `window` : 문장 내에서 현재 단어와 예측 단어 사이의 최대 거리. ex) 타겟 단어의 왼쪽과 오른쪽 n번째 단어\n",
    "- `vector_size` : 단어 벡터의 차원 수\n",
    "- `min_count` : 이 값보다 총 절대 빈도수가 낮은 모든 단어를 무시함 - (2, 100)\n",
    "- `sg` : 1은 skip-gram, 0은 CBOW method를 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 정의 하기\n",
    "w2v_model = Word2Vec(min_count=20,\n",
    "                     window=2,\n",
    "                     vector_size=300,\n",
    "                     sample=6e-5, \n",
    "                     alpha=0.03, \n",
    "                     min_alpha=0.0007)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문장에 들어있는 각 단어들을 Word2Vec 모델이 인식할 수 있는 형태로 변환\n",
    "w2v_model.build_vocab(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19990485, 54001900)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모델 훈련\n",
    "w2v_model.train(sentences, total_examples=w2v_model.corpus_count, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method train in module gensim.models.word2vec:\n",
      "\n",
      "train(corpus_iterable=None, corpus_file=None, total_examples=None, total_words=None, epochs=None, start_alpha=None, end_alpha=None, word_count=0, queue_factor=2, report_delay=1.0, compute_loss=False, callbacks=(), **kwargs) method of gensim.models.word2vec.Word2Vec instance\n",
      "    Update the model's neural weights from a sequence of sentences.\n",
      "    \n",
      "    Notes\n",
      "    -----\n",
      "    To support linear learning-rate decay from (initial) `alpha` to `min_alpha`, and accurate\n",
      "    progress-percentage logging, either `total_examples` (count of sentences) or `total_words` (count of\n",
      "    raw words in sentences) **MUST** be provided. If `sentences` is the same corpus\n",
      "    that was provided to :meth:`~gensim.models.word2vec.Word2Vec.build_vocab` earlier,\n",
      "    you can simply use `total_examples=self.corpus_count`.\n",
      "    \n",
      "    Warnings\n",
      "    --------\n",
      "    To avoid common mistakes around the model's ability to do multiple training passes itself, an\n",
      "    explicit `epochs` argument **MUST** be provided. In the common and recommended case\n",
      "    where :meth:`~gensim.models.word2vec.Word2Vec.train` is only called once, you can set `epochs=self.epochs`.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    corpus_iterable : iterable of list of str\n",
      "        The ``corpus_iterable`` can be simply a list of lists of tokens, but for larger corpora,\n",
      "        consider an iterable that streams the sentences directly from disk/network, to limit RAM usage.\n",
      "        See :class:`~gensim.models.word2vec.BrownCorpus`, :class:`~gensim.models.word2vec.Text8Corpus`\n",
      "        or :class:`~gensim.models.word2vec.LineSentence` in :mod:`~gensim.models.word2vec` module for such examples.\n",
      "        See also the `tutorial on data streaming in Python\n",
      "        <https://rare-technologies.com/data-streaming-in-python-generators-iterators-iterables/>`_.\n",
      "    corpus_file : str, optional\n",
      "        Path to a corpus file in :class:`~gensim.models.word2vec.LineSentence` format.\n",
      "        You may use this argument instead of `sentences` to get performance boost. Only one of `sentences` or\n",
      "        `corpus_file` arguments need to be passed (not both of them).\n",
      "    total_examples : int\n",
      "        Count of sentences.\n",
      "    total_words : int\n",
      "        Count of raw words in sentences.\n",
      "    epochs : int\n",
      "        Number of iterations (epochs) over the corpus.\n",
      "    start_alpha : float, optional\n",
      "        Initial learning rate. If supplied, replaces the starting `alpha` from the constructor,\n",
      "        for this one call to`train()`.\n",
      "        Use only if making multiple calls to `train()`, when you want to manage the alpha learning-rate yourself\n",
      "        (not recommended).\n",
      "    end_alpha : float, optional\n",
      "        Final learning rate. Drops linearly from `start_alpha`.\n",
      "        If supplied, this replaces the final `min_alpha` from the constructor, for this one call to `train()`.\n",
      "        Use only if making multiple calls to `train()`, when you want to manage the alpha learning-rate yourself\n",
      "        (not recommended).\n",
      "    word_count : int, optional\n",
      "        Count of words already trained. Set this to 0 for the usual\n",
      "        case of training on all words in sentences.\n",
      "    queue_factor : int, optional\n",
      "        Multiplier for size of queue (number of workers * queue_factor).\n",
      "    report_delay : float, optional\n",
      "        Seconds to wait before reporting progress.\n",
      "    compute_loss: bool, optional\n",
      "        If True, computes and stores loss value which can be retrieved using\n",
      "        :meth:`~gensim.models.word2vec.Word2Vec.get_latest_training_loss`.\n",
      "    callbacks : iterable of :class:`~gensim.models.callbacks.CallbackAny2Vec`, optional\n",
      "        Sequence of callbacks to be executed at specific stages during training.\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    .. sourcecode:: pycon\n",
      "    \n",
      "        >>> from gensim.models import Word2Vec\n",
      "        >>> sentences = [[\"cat\", \"say\", \"meow\"], [\"dog\", \"say\", \"woof\"]]\n",
      "        >>>\n",
      "        >>> model = Word2Vec(min_count=1)\n",
      "        >>> model.build_vocab(sentences)  # prepare the model vocabulary\n",
      "        >>> model.train(sentences, total_examples=model.corpus_count, epochs=model.epochs)  # train word vectors\n",
      "        (1, 30)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(w2v_model.train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 단어간 유사도 확인하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dir(w2v_model.wv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- most_similar : 주어진 조건에 가장 적합한 단어 탐색\n",
    "- similarity : 주어진 단어들의 유사도 계산\n",
    "- doesnt_match : 주어진 단어들 중 가장 '덜 유사한' 단어"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# help(w2v_model.wv.most_similar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# help(w2v_model.wv.similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('marge', 0.39361003041267395),\n",
       " ('son', 0.3050530254840851),\n",
       " ('simpson', 0.2926293909549713),\n",
       " ('bart', 0.28149035573005676),\n",
       " ('mr', 0.2747665047645569),\n",
       " ('husband', 0.2533870339393616),\n",
       " ('lisa', 0.24030055105686188),\n",
       " ('dad', 0.22668324410915375),\n",
       " ('smither', 0.22446727752685547),\n",
       " ('homie', 0.21520781517028809)]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.most_similar(positive=[\"homer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('lisa', 0.4804288148880005),\n",
       " ('dad', 0.3908121585845947),\n",
       " ('boy', 0.35573628544807434),\n",
       " ('mom', 0.34492626786231995),\n",
       " ('son', 0.281699538230896),\n",
       " ('homer', 0.28149035573005676),\n",
       " ('milhouse', 0.2809959948062897),\n",
       " ('mother', 0.2730790376663208),\n",
       " ('maggie', 0.2726394832134247),\n",
       " ('child', 0.27139681577682495)]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.most_similar(positive=[\"bart\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Woman : homer = ___ : marge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('man', 0.2424401193857193),\n",
       " ('young', 0.22867713868618011),\n",
       " ('see', 0.2077772170305252)]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.most_similar(positive=[\"woman\", \"homer\"], negative=[\"marge\"], topn=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('mom', 0.2672269642353058),\n",
       " ('lisa', 0.26112234592437744),\n",
       " ('embarrassing', 0.2553335726261139)]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.most_similar(positive=[\"woman\", \"bart\"], negative=[\"man\"], topn=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bart'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.doesnt_match(['bart', 'homer', 'marge'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'marge'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.doesnt_match(['bart', 'lisa', 'marge'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 단어 임베딩의 한계점"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "bank_vector = w2v_model.wv.get_vector(w2v_model.wv.key_to_index['bank'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bank_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bank_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 우리가 사용하는 모든 단어는 context에 따라 의미가 다르다\n",
    "- 단어 embedding의 경우 이런 유연성을 확보하지 못 함\n",
    "    - 배를 깎아 먹었다 / 배가 고프다 / 배 멀미를 하다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bank_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sentence embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre-trained model tokenizer와 and bert model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased') # smaller & uncased model\n",
    "model = BertModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bank가 들어간 유사한 문장 두 개\n",
    "sentence1 = \"I deposited money at the bank.\"\n",
    "sentence2 = \"The ducks swam to the river bank.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문장을 BERT가 인식할 수 있는 형태로 Tokenize\n",
    "encoded_input1 = tokenizer(sentence1, return_tensors='pt') # pytorch\n",
    "encoded_input2 = tokenizer(sentence2, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,  1045, 14140,  2769,  2012,  1996,  2924,  1012,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_input1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `input_ids` : 각 단어별로 매핑된 key. 101은 문장의 시작을, 102는 문장의 끝을 의미\n",
    "- `token_type_ids` : 문장 번호\n",
    "- `attention_mask` : attention을 가져야 하는 단어는 1, 그렇지 않은 단어는 0. (만약 input이 실제 단어들이라면 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding 생성!\n",
    "with torch.no_grad():\n",
    "    output1 = model(**encoded_input1)\n",
    "    output2 = model(**encoded_input2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding 내에서 bank라는 단어 찾아오기 (문장의 5번째에 있는 단어)\n",
    "bank_embedding_sentence1 = output1.last_hidden_state[0, 5, :]\n",
    "bank_embedding_sentence2 = output2.last_hidden_state[0, 5, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity between the two embeddings: 0.59224105\n"
     ]
    }
   ],
   "source": [
    "# cosine similarity 계산\n",
    "\n",
    "similarity = cosine_similarity(bank_embedding_sentence1, bank_embedding_sentence2)\n",
    "# print(\"Embedding for 'bank' in sentence 1:\", bank_embedding_sentence1)\n",
    "# print(\"Embedding for 'bank' in sentence 2:\", bank_embedding_sentence2)\n",
    "print(\"Cosine similarity between the two embeddings:\", similarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--END--"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lang2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
