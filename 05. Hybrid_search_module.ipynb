{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "import base64\n",
    "import requests\n",
    "from PIL import Image\n",
    "import openai\n",
    "import os\n",
    "import numpy as np\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 목차\n",
    "\n",
    "#### 1. GPT4v를 이용하여 우리들이 갖고 있는 attribute에 맞게 특징 추출 -> 텍스트로 변환\n",
    "#### 2. Text search : top-100 (broad search)\n",
    "#### 3. Image search를 활용한 2차 서치"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enrich image with descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize openai\n",
    "os.environ['OPENAI_API_KEY']= \"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\"\n",
    "openai.api_key = os.environ[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "\"'main_category', 'silhouette', 'silhouette_fit', 'waistline',\n",
    "       'length', 'collar_type', 'neckline_type', 'sleeve_type',\n",
    "       'pocket_type', 'opening_type', 'non-textile material type',\n",
    "       'leather', 'textile finishing, manufacturing techniques',\n",
    "       'textile pattern', 'animal', 'other'\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from search_utils import clothes_detector\n",
    "from transformers import YolosFeatureExtractor, YolosForObjectDetection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"valentinafeve/yolos-fashionpedia\"\n",
    "\n",
    "feature_extractor = YolosFeatureExtractor.from_pretrained('hustvl/yolos-small')\n",
    "model = YolosForObjectDetection.from_pretrained(MODEL_NAME)\n",
    "\n",
    "image = Image.open(\"test_images/test_image5.jpg\").convert(\"RGB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cropped_images = clothes_detector(image, feature_extractor, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cropped_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cropped_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cropped_images['shoe']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize openai\n",
    "os.environ['OPENAI_API_KEY']= \"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\"\n",
    "openai.api_key = os.environ[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_image(image_path):\n",
    "  with open(image_path, \"rb\") as image_file:\n",
    "    return base64.b64encode(image_file.read()).decode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import base64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT를 이용해서 이미지를 읽어와서 description을 생성한다\n",
    "\n",
    "def describe_clothes(image, label, openai_key):\n",
    "  buffer = io.BytesIO()\n",
    "  # Save the image to the buffer in JPEG format\n",
    "  image.save(buffer, format=\"JPEG\")\n",
    "  buffer.seek(0)\n",
    "  image_data = buffer.read()\n",
    "\n",
    "  base64_image = base64.b64encode(image_data).decode('utf-8')\n",
    "  image_desc_prompt = \"\"\"Focus on {} inside the image.\n",
    "        Identify the attributes of the item.\n",
    "        The attributes you should answer are : \n",
    "        - clothes_type\n",
    "        - color\n",
    "        - silhouette\n",
    "        - silhouette_fit\n",
    "        - waisteline\n",
    "        - sleeve_type\n",
    "        - collar_type\n",
    "        - length\n",
    "        - gender\n",
    "        - patterns\n",
    "        - textile_pattern\n",
    "        \n",
    "        Ignore the attributes you cannot answer.\n",
    "        Keep the answer simple and clear, having max three words per attribute.\n",
    "  \"\"\".format(label)\n",
    "\n",
    "  headers = {\n",
    "    \"Content-Type\": \"application/json\",\n",
    "    \"Authorization\": f\"Bearer {openai_key}\"\n",
    "  }\n",
    "\n",
    "  payload = {\n",
    "    \"model\": \"gpt-4-vision-preview\",\n",
    "    \"messages\": [\n",
    "      {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "          {\n",
    "            \"type\": \"text\",\n",
    "            \"text\": image_desc_prompt\n",
    "          },\n",
    "          {\n",
    "            \"type\": \"image_url\",\n",
    "            \"image_url\": {\n",
    "              \"url\": f\"data:image/jpeg;base64,{base64_image}\"\n",
    "            }\n",
    "          }\n",
    "        ]\n",
    "      }\n",
    "    ],\n",
    "    \"max_tokens\": 300\n",
    "  }\n",
    "\n",
    "  response = requests.post(\"https://api.openai.com/v1/chat/completions\", headers=headers, json=payload)\n",
    "\n",
    "  return response.json()['choices'][0]['message']['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cropped_images.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptions = dict()\n",
    "\n",
    "for i, img in cropped_images.items():\n",
    "    print(i)\n",
    "    desc = describe_clothes(img, i, openai.api_key)\n",
    "    descriptions[i] = desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from search_utils import fashion_query_transformer, text_search\n",
    "from image_utils import draw_images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다음과 같은 방법으로 text 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_query = fashion_query_transformer(str(descriptions))\n",
    "text_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from splade.splade.models.transformer_rep import Splade\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "splade_model_id = 'naver/splade-cocondenser-ensembledistil'\n",
    "\n",
    "splade_model = Splade(splade_model_id, agg='max')\n",
    "splade_model.to('cpu')\n",
    "splade_model.eval()\n",
    "\n",
    "splade_tokenizer = AutoTokenizer.from_pretrained(splade_model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone import Pinecone\n",
    "\n",
    "pc = Pinecone(api_key=\"74e30e50-02fa-4e55-9bff-affa6a3817a0\")\n",
    "# index 개수 확인\n",
    "# index_list = pc.list_indexes().indexes\n",
    "\n",
    "# index description\n",
    "index = pc.Index(\"fastcampus\")\n",
    "# index.describe_index_stats()\n",
    "\n",
    "# CLIP\n",
    "from image_utils import fetch_clip, extract_img_features, draw_images\n",
    "\n",
    "model, processor, tokenizer = fetch_clip(model_name=\"patrickjohncyh/fashion-clip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = text_search(index, text_query, model, tokenizer, splade_model, splade_tokenizer, top_k=100, hybrid=False)\n",
    "results.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(results['tights, stockings'].matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = dict()\n",
    "for k,v in results.items():\n",
    "    paths[k] = [i['metadata']['img_path'] for i in v['matches']]\n",
    "\n",
    "# 이미지들 show\n",
    "for k,v in paths.items():\n",
    "    print(k)\n",
    "    draw_images([Image.open(i) for i in v[:10]]) # 10개씩만 display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 매칭된 이미지 내에서 한 번 더 서치를 진행 (image search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_db = pd.read_csv(\"local_db.csv\")\n",
    "local_db['values'] = local_db['values'].apply(json.loads)\n",
    "local_db.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_db.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "각 카테고리별로 서치된 결과 값을 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = list()\n",
    "\n",
    "for category, value in results.items():\n",
    "    id = [i['id'] for i in value['matches']]\n",
    "    ids.append({category:id})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids[0].keys(), list(ids[0].values())[0][:3], \"...\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids[1]['shoe']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids[0]['tights, stockings']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이미지를 갖고 embedding화 한다\n",
    "# cropped_images 도 필요\n",
    "\n",
    "final_results = list()\n",
    "\n",
    "for search_result in ids:\n",
    "    category = list(search_result.keys())[0]\n",
    "    search_ids = list(search_result.values())[0]\n",
    "    # 관련 \n",
    "    filtered_local_db = local_db.loc[local_db['vdb_id'].isin(search_ids)]\n",
    "    \n",
    "    img_emb = extract_img_features(cropped_images[category], processor, model)\n",
    "\n",
    "    # def search_local_db()\n",
    "    def calculate_dot_products(embedding, df, column_name):\n",
    "        dot_products = df[column_name].apply(lambda x: np.dot(embedding, x))\n",
    "        return dot_products\n",
    "\n",
    "    # Calculate dot products\n",
    "    dot_products = calculate_dot_products(img_emb.cpu().numpy()[0], filtered_local_db, 'values')\n",
    "\n",
    "    # Find the indices of the top 5 most similar embeddings\n",
    "    top_indices = dot_products.nlargest(10).index\n",
    "\n",
    "    # Retrieve the top 5 most similar embeddings\n",
    "    top_similar_ids = filtered_local_db.loc[top_indices, 'vdb_id'].tolist()\n",
    "    \n",
    "    final_results.append({category:top_similar_ids})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cropped_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_local_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for search_result in final_results:\n",
    "    category = list(search_result.keys())[0]\n",
    "    paths = list(search_result.values())[0]\n",
    "\n",
    "    full_paths = [os.path.join(\"imaterialist-fashion-2020-fgvc7\", \"cropped_images\", i+\".jpg\") for i in paths]\n",
    "    print(category)\n",
    "    draw_images([Image.open(i) for i in full_paths])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function으로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from search_utils import get_single_text_embedding\n",
    "\n",
    "def calculate_dot_products(embedding, df, column_name):\n",
    "    dot_products = df[column_name].apply(lambda x: np.dot(embedding, x))\n",
    "    return dot_products\n",
    "\n",
    "\n",
    "def get_top_indices(db, input_data, category, clip_processor, clip_model, clip_tokenizer, top_k, type='image'):\n",
    "    if type=='image':\n",
    "        # input_data should be a single cropped image\n",
    "        emb = extract_img_features(input_data, clip_processor, clip_model)\n",
    "        # Calculate dot products\n",
    "        dot_products = calculate_dot_products(emb.cpu().numpy()[0], db, 'values')\n",
    "    elif type=='text':\n",
    "        # input_data should be a single string of text\n",
    "        emb = get_single_text_embedding(input_data, clip_model, clip_tokenizer)\n",
    "        # Calculate dot products\n",
    "        dot_products = calculate_dot_products(np.array(emb)[0], db, 'values')\n",
    "\n",
    "    # Find the indices of the top 5 most similar embeddings\n",
    "    top_indices = dot_products.nlargest(top_k).index\n",
    "\n",
    "    # Retrieve the top 5 most similar embeddings\n",
    "    top_similar_ids = db.loc[top_indices, 'vdb_id'].tolist()\n",
    "\n",
    "    return {category:top_similar_ids}\n",
    "\n",
    "\n",
    "def additional_search(local_db, cropped_images, search_results, clip_processor, clip_model, clip_tokenizer, top_k=10):\n",
    "    \n",
    "    ids = list()\n",
    "    for category, value in search_results.items():\n",
    "        id = [i['id'] for i in value['matches']]\n",
    "        ids.extend(id)\n",
    "\n",
    "    final_results = list()\n",
    "\n",
    "    # 전체 아이템 중, 1차 retrieve 된 것들만 가져옴\n",
    "    db = local_db.loc[local_db['vdb_id'].isin(ids)]\n",
    "\n",
    "    for label, v in search_results.items(): # 텍스트로부터\n",
    "        tmp = db.loc[db['name']==label]\n",
    "\n",
    "        # 텍스트에도 있고, 이미지에도 레이블이 있는 경우\n",
    "        if label in cropped_images.keys():\n",
    "            r = get_top_indices(tmp, cropped_images[label], label, clip_processor, clip_model, clip_tokenizer, top_k, type='image')\n",
    "            final_results.append(r)\n",
    "        # 텍스트에만 있는 경우, 그냥 top_k를 가져온다\n",
    "        else:\n",
    "            final_results.append({ label : [i['id'] for i in v['matches']][:top_k]} )\n",
    "\n",
    "    refined_result = dict()\n",
    "\n",
    "    for search_result in final_results:\n",
    "        category = list(search_result.keys())[0]\n",
    "        paths = list(search_result.values())[0]\n",
    "\n",
    "        full_paths = [os.path.join(\"imaterialist-fashion-2020-fgvc7\", \"cropped_images\", i+\".jpg\") for i in paths]\n",
    "        refined_result[category] = full_paths\n",
    "\n",
    "        \n",
    "    return refined_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize openai\n",
    "os.environ['OPENAI_API_KEY']= \"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\"\n",
    "openai.api_key = os.environ[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"valentinafeve/yolos-fashionpedia\"\n",
    "\n",
    "feature_extractor = YolosFeatureExtractor.from_pretrained('hustvl/yolos-small')\n",
    "model = YolosForObjectDetection.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_model, clip_processor, clip_tokenizer = fetch_clip(model_name=\"patrickjohncyh/fashion-clip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from search_utils import clothes_detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = Image.open(\"test_images/test.jpg\")\n",
    "# image = fix_channels(ToTensor()(image))\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cropped_images = clothes_detector(image, feature_extractor, model, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cropped_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptions = dict()\n",
    "\n",
    "for i, img in cropped_images.items():\n",
    "    print(i)\n",
    "    desc = describe_clothes(img, i, openai.api_key)\n",
    "    descriptions[i] = desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, v in descriptions.items():\n",
    "    print(i)\n",
    "    print(v)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_query = fashion_query_transformer(str(descriptions))\n",
    "text_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = text_search(index, text_query, clip_model, clip_tokenizer, splade_model, splade_tokenizer, top_k=100, hybrid=False)\n",
    "results.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = dict()\n",
    "for k,v in results.items():\n",
    "    paths[k] = [i['metadata']['img_path'] for i in v['matches']]\n",
    "\n",
    "# 이미지들 show\n",
    "for k,v in paths.items():\n",
    "    print(k)\n",
    "    draw_images([Image.open(i) for i in v[:10]]) # 10개씩만 display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_results = additional_search(local_db, cropped_images, results, clip_processor, clip_model, clip_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k,v in final_results.items():\n",
    "    print(k)\n",
    "    draw_images([Image.open(i) for i in v])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastcampus",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
