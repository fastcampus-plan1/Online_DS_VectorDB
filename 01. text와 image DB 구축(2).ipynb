{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import torch\n",
    "import json\n",
    "import itertools\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import os\n",
    "import openai\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. 왜 sparse vector를 사용할까?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from text_utils import create_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize openai\n",
    "os.environ['OPENAI_API_KEY']= \"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\"\n",
    "openai.api_key = os.environ[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from splade.splade.models.transformer_rep import Splade\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "sparse_model_id = 'naver/splade-cocondenser-ensembledistil'\n",
    "\n",
    "# splade = 'naver/splade-v3'\n",
    "sparse_model = Splade(sparse_model_id, agg='max')\n",
    "# sparse_model.to('cpu')  # move to GPU if possible\n",
    "sparse_model.eval()\n",
    "\n",
    "splade_tokenizer = AutoTokenizer.from_pretrained(sparse_model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_sparse_vector(text):\n",
    "    tokens = splade_tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        sparse_emb = sparse_model(\n",
    "            d_kwargs=tokens.to('cpu')\n",
    "        )['d_rep'].squeeze()\n",
    "\n",
    "    return sparse_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"pants\"\n",
    "# input_text = \"low waist\"\n",
    "# input_text = \"trousers\" # (pants의 동의어)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\n",
    "   \"pants, silhouette_name : straight, symmetrical,silhouette_fit_name : regular (fit), waistline_name : low waist,\",\n",
    "   \"pants, silhouette_name : straight, symmetrical,silhouette_fit_name : regular (fit), geometric pattern\",\n",
    "   \"symmetrical, silhouette_fit_name : regular (fit), waistline_name : low waist,\",\n",
    "   \"symmetrical, silhouette_fit_name : regular (fit), waistline_name : high waist,\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embs = create_embeddings(texts)\n",
    "input_emb = create_embeddings([input_text])\n",
    "\n",
    "print(\"Dense vector similarities :\", cosine_similarity(input_emb, embs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in texts:\n",
    "    print('-', i)\n",
    "\n",
    "print()\n",
    "s_embs = [gen_sparse_vector(t) for t in texts]\n",
    "input_s_emb = [gen_sparse_vector(input_text)]\n",
    "\n",
    "print(\"Sparse vector similarities :\", cosine_similarity(input_s_emb, s_embs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 각 단어별 weight 살펴보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the tokens that will be input into the model\n",
    "tokens = splade_tokenizer(texts[0], return_tensors=\"pt\")\n",
    "splade_tokenizer.convert_ids_to_tokens(tokens['input_ids'][0])\n",
    "\n",
    "with torch.no_grad():\n",
    "    sparse_emb = sparse_model(\n",
    "        d_kwargs=tokens.to('cpu')\n",
    "    )['d_rep'].squeeze()\n",
    "sparse_emb.shape\n",
    "\n",
    "indices = sparse_emb.nonzero().squeeze().cpu().tolist()\n",
    "values = sparse_emb[indices].cpu().tolist()\n",
    "\n",
    "print(len(indices))\n",
    "\n",
    "idx2token = {idx: token for token, idx in splade_tokenizer.get_vocab().items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"pants, silhouette_name : straight, symmetrical,silhouette_fit_name : regular (fit), waistline_name : low waist,\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_dict_tokens = {\n",
    "    idx2token[idx]: round(weight, 2) for idx, weight in zip(indices, values)\n",
    "}\n",
    "# sort so we can see most relevant tokens first\n",
    "sparse_dict_tokens = {\n",
    "    k: v for k, v in sorted(\n",
    "        sparse_dict_tokens.items(),\n",
    "        key=lambda item: item[1],\n",
    "        reverse=True\n",
    "    )\n",
    "}\n",
    "# sparse_dict_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 목차 - sparse vector\n",
    "\n",
    "#### 1. 각 supercategory 별로 group\n",
    "#### 2. 하나의 document로 변환\n",
    "#### 3. SPLADE\n",
    "#### 4. pineconeDB upsert 형태로 변환\n",
    "\n",
    "\n",
    "- document : \n",
    "\n",
    "```json\n",
    "silhouette_name : symmetrical,\n",
    "silhouette_fit_name : regular (fit),\n",
    "waistline_name : low waist,\n",
    "length_name : maxi (length),\n",
    "opening_type_name : fly (opening),\n",
    "non-textile material type_name : no non-textile material\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def listify(string, encap_type=\"()\"):\n",
    "    return [int(num) for num in string.strip(encap_type).split(', ')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attributes = pd.read_csv(\"attribute_specific.csv\")\n",
    "new_df = pd.read_csv(\"clothes_final2.csv\")\n",
    "\n",
    "new_df['bbox'] = [listify(i, \"[]\") for i in new_df['bbox']]\n",
    "new_df['bbox_big'] = [listify(i, \"[]\") for i in new_df['bbox_big']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df['name'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supercategory 별로 attribute를 구분하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 조금 더 자세히 살펴보면, 우리들이 갖고 있는 attribute은 몇 가지 레벨로 나눌 수 있다\n",
    "\t- 가장 큰 단위는 당연히 classID - 바지, 상의, 신발 등\n",
    "\t- 더 세밀하게 나눠보면 classID에 따른 특징들을 그룹화 할 수 있다\n",
    "\t\t- 하위 attribute들은 각자 다른 특징을 나타낸다\n",
    "\t\t- 핏감, 전체적인 옷의 형태, 질감, 마감, 길이 등\n",
    "\t\t- 따라서 이런 attribute들을 모두 각자의 그룹에 맞게 고려되어야 한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attributes.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attributes['supercategory2'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아무런 attribute이 없는 항목은 'normal'이라는 attribute을 임의로 부여"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.loc[new_df['AttributesIds'].isna(), 'AttributesIds'] = \"999\"\n",
    "new_df.loc[new_df['AttributesNames'].isna(), 'AttributesNames'] = \"normal\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.tail(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Main attribute과 secondary attribute들을 하나로 묶음\n",
    "    - 이는 우리가 supercategory2를 임의로 지정하여 sleeve, collar들에 해당하는 attribute ID를 별도로 처리할 수 있기 때문"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_columns(row):\n",
    "    # Check if either value is np.nan and return the other value in such cases\n",
    "    if pd.isna(row['AttributesIds']) and not pd.isna(row['second_AttributesIds']):\n",
    "        return row['second_AttributesIds']\n",
    "    elif not pd.isna(row['AttributesIds']) and pd.isna(row['second_AttributesIds']):\n",
    "        return row['AttributesIds']\n",
    "    elif pd.isna(row['AttributesIds']) and pd.isna(row['second_AttributesIds']):\n",
    "        return np.nan\n",
    "    else:\n",
    "        # Both values are not np.nan, merge with a comma\n",
    "        return f\"{row['AttributesIds']},{row['second_AttributesIds']}\"\n",
    "\n",
    "new_df['AttributesIds_merged'] = new_df.apply(merge_columns, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert2list(string):\n",
    "    if pd.isna(string):\n",
    "        return np.nan\n",
    "    else:\n",
    "        return list(set([i for i in string.split(',')]))\n",
    "\n",
    "new_df['AttributesIds_list'] = new_df['AttributesIds_merged'].apply(convert2list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 각 attribute들을 분류하여 각 컬럼에 배치\n",
    "- `attributes`를 참고하며, 각 attribute ID를 attribute name으로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attributes.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attributes['id'] = attributes['id'].astype(str)\n",
    "\n",
    "# Create a mapping of id to supercategory2\n",
    "id_to_supercategory2 = attributes.set_index('id')['supercategory2'].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_to_supercategory2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 카테고리별로 list를 만든다\n",
    "category_distributions = list()\n",
    "\n",
    "# row를 루프를 돌면서 각 dictionary value에 채워 넣는다\n",
    "for idx, row in tqdm(new_df.iterrows()):\n",
    "    tmp_dict = {k:'' for k in attributes['supercategory2'].unique()}\n",
    "    for attr in row['AttributesIds_list']:\n",
    "        supercat_type = id_to_supercategory2[attr]\n",
    "        if tmp_dict[supercat_type]=='':\n",
    "            tmp_dict[supercat_type] += attr\n",
    "        else:\n",
    "            tmp_dict[supercat_type] += \",\" + attr\n",
    "        # break\n",
    "    category_distributions.append(tmp_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_distributions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 dictionary를 dataframe 형태로 변환\n",
    "category_dist_df = [pd.DataFrame([d]) for d in category_distributions]\n",
    "# list of dataframe을 하나의 dataframe으로 concat\n",
    "category_dist_df = pd.concat(category_dist_df, axis=0)\n",
    "# reset index\n",
    "category_dist_df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "category_dist_df = category_dist_df.replace('', np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_dist_df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 기존의 데이터셋과 결합"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = pd.concat([new_df, category_dist_df], axis=1)\n",
    "new_df = new_df.replace('', np.nan)\n",
    "new_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document 형태로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_dist_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_to_name = pd.Series(attributes.name.values, index=attributes.id).to_dict()\n",
    "\n",
    "# Define a function to convert IDs to names\n",
    "def ids_to_names(ids, id_to_name=id_to_name):\n",
    "    if pd.isna(ids):\n",
    "        return np.nan\n",
    "    names = [id_to_name.get(id_, 'Unknown') for id_ in ids.split(',')]\n",
    "    return ', '.join(names)\n",
    "\n",
    "\n",
    "for col in category_dist_df.columns:\n",
    "    if 'name' not in col:\n",
    "        category_dist_df[col+\"_name\"] = category_dist_df[col].apply(ids_to_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_dist_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# human-readable한 값들로 변환된 컬럼들을 활용\n",
    "named_df = category_dist_df[[i for i in category_dist_df.columns if '_name' in i]]\n",
    "\n",
    "# 각 row 별로 하나의 string 값으로 변환\n",
    "def row_to_string(row):\n",
    "    return ',\\n'.join([f\"{col} : {row[col]}\" for col in named_df.columns if pd.notna(row[col])])\n",
    "\n",
    "# Applying the function to each row of the DataFrame and storing the results in a list\n",
    "list_of_strings = named_df.apply(row_to_string, axis=1).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list_of_strings[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df['doc'] = list_of_strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 로컬에 저장\n",
    "# new_df.to_csv(\"clothes_final_sparse_doc.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = \"imaterialist-fashion-2020-fgvc7/cropped_images/\"\n",
    "\n",
    "new_df['img_path'] = base_path + new_df['ImageId'].astype(str) + \"_\" + new_df['entity_id'].astype(str) + \".jpg\"\n",
    "# image df와의 join을 위한 키 생성\n",
    "new_df['img_id'] = new_df['ImageId'].astype(str) + \"_\" + new_df['entity_id'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.loc[2022, 'img_path']\n",
    "\n",
    "Image.open(\"imaterialist-fashion-2020-fgvc7/train/054f0ae9527a9a79a4de6f3acc166e5b.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 2020\n",
    "print(new_df.loc[i, 'name'])\n",
    "print(new_df.loc[i, 'doc'])\n",
    "Image.open(new_df.loc[i, 'img_path'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 2022\n",
    "print(new_df.loc[i, 'name'])\n",
    "print(new_df.loc[i, 'doc'])\n",
    "Image.open(new_df.loc[i, 'img_path'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = pd.read_csv(\"clothes_final_sparse_doc.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 이미지 embeddings와 함께 merge하여 하나의 dataframe으로 결합"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = {}\n",
    "\n",
    "with open('img_embeddings_fashion_fine_tuned.json', 'r') as file:\n",
    "    for line in file:\n",
    "        # Convert each line to a dictionary\n",
    "        embedding_dict = json.loads(line.strip())\n",
    "        \n",
    "        # Convert the list back to a NumPy array if necessary\n",
    "        for img_name, emb_list in embedding_dict.items():\n",
    "            embeddings[img_name] = np.array(emb_list)\n",
    "\n",
    "image_embedddings = pd.DataFrame([embeddings]).T.reset_index()\n",
    "image_embedddings.rename(columns={\"index\":\"img_id\", 0:\"img_emb\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_embedddings.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = pd.merge(new_df, image_embedddings, on='img_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모두 잘 join 되었는지 확인\n",
    "new_df.img_emb.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CLIP : SPLADE = Dense : sparse vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- hybrid search를 위해서는 dense vector와 sparse vector를 짝을 지어줘야 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from splade.splade.models.transformer_rep import Splade\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "sparse_model_id = 'naver/splade-cocondenser-ensembledistil'\n",
    "\n",
    "# splade = 'naver/splade-v3'\n",
    "sparse_model = Splade(sparse_model_id, agg='max')\n",
    "sparse_model.to('cpu')  # move to GPU if possible\n",
    "sparse_model.eval()\n",
    "\n",
    "splade_tokenizer = AutoTokenizer.from_pretrained(sparse_model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upsert 형식\n",
    "\n",
    "```json\n",
    "{\"id\" : \"0838a48a7b0bfa789a5181ab0e8f4ee2_3040\", # 이미지 파일 이름 + entity ID\n",
    " \"values\" : [-0.08405803143978119, -0.7088879346847534, ...], # CLIP embeddings\n",
    " \"sparse_values\" : {\n",
    "    \"indices\" : [1045, 1062, ...], # non-zero index\n",
    "    \"values\" : [1.3038887977600098, 0.304147332906723, ...] # non-zero values\n",
    "    },\n",
    "\"metadata\" : {\n",
    "    # 이미지 파일 path\n",
    "    \"img_path\": \"imaterialist-fashion-2020-fgvc7/cropped_images/0838a48a7b0bfa789a5181ab0e8f4ee2_3040.jpg\",\n",
    "    \"category\": \"coat\"\n",
    "} \n",
    "}\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_sparse_vector(text):\n",
    "    tokens = splade_tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        sparse_emb = sparse_model(\n",
    "            d_kwargs=tokens.to('cpu')\n",
    "        )['d_rep'].squeeze()\n",
    "    \n",
    "    indices = sparse_emb.nonzero().squeeze().cpu().tolist()\n",
    "    values = sparse_emb[indices].cpu().tolist()\n",
    "\n",
    "    return indices, values\n",
    "\n",
    "def upsert_format(id, text, img_emb):\n",
    "    index, value = gen_sparse_vector(text)\n",
    "    \n",
    "    sparse_values = {\n",
    "        \"indices\": index,\n",
    "        \"values\": value\n",
    "    }\n",
    "    \n",
    "    upsert = {\n",
    "        \"id\": id,\n",
    "        \"values\": img_emb,\n",
    "        \"sparse_values\":sparse_values,\n",
    "        \"metadata\":{\"img_path\":\"imaterialist-fashion-2020-fgvc7/cropped_images/\"+id+\".jpg\"}\n",
    "    }\n",
    "    return upsert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = new_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upserts = list()\n",
    "\n",
    "for _, row in tqdm(tmp.iterrows(), total=tmp.shape[0]):\n",
    "    upserts.append(upsert_format(row['img_id'], row['doc'], row['img_emb'].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(upserts[0]['values'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(upserts[0]['sparse_values'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upserts[0]['sparse_values'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upserts[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upserts[0]['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upserts[0]['metadata']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upserts[0]['sparse_values'].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`01. generate_SPLADE_embeddings.ipynb` 참고"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 만들어진 sparse vector 읽어오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_read = []\n",
    "\n",
    "# Open the file in read mode\n",
    "with open(\"upsert_vectors_fashion_fine_tuned.json\", 'r') as file:\n",
    "    # Iterate through each line in the file\n",
    "    for line in file:\n",
    "        # Parse the JSON string into a Python dictionary\n",
    "        data = json.loads(line)\n",
    "        # Append the dictionary to the list\n",
    "        data_read.append(data)\n",
    "\n",
    "# Now, data_read contains all the dictionaries read from the file\n",
    "print(f\"Successfully read {len(data_read)} items from upsert_vectors_fashion_fine_tuned.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_read[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_read[0]['metadata']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastcampus",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
